"""
Agent IA LangChain pour la prise de d√©cision d'irrigation
"""
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_core.messages import HumanMessage, SystemMessage
from typing import Dict
import os
import json
import requests
from config import OPENAI_API_KEY, LLM_MODEL, TEMPERATURE, LLM_PROVIDER, OLLAMA_BASE_URL


class IrrigationAgent:
    """Agent IA utilisant LangChain pour prendre des d√©cisions d'irrigation"""
    
    def __init__(self):
        """Initialise l'agent avec le mod√®le LLM"""
        
        if LLM_PROVIDER == 'ollama':
            print(f"Utilisation de Ollama avec le modele {LLM_MODEL}")
            try:
                # V√©rifier la disponibilit√© d'Ollama et des mod√®les
                import requests
                try:
                    response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=5)
                    if response.status_code == 200:
                        models = response.json().get('models', [])
                        model_names = [m.get('name', '') for m in models]
                        if model_names:
                            print(f"[INFO] Modeles Ollama disponibles : {', '.join(model_names)}")
                        else:
                            print("[WARNING] Aucun modele Ollama trouve. Assurez-vous d'avoir telecharge un modele avec 'ollama pull <nom_modele>'")
                        
                        # V√©rifier si le mod√®le existe (avec ou sans tag :latest)
                        model_found = LLM_MODEL in model_names
                        if not model_found:
                            # Essayer avec :latest
                            model_with_latest = f"{LLM_MODEL}:latest"
                            if model_with_latest in model_names:
                                print(f"[INFO] Modele '{LLM_MODEL}' trouve comme '{model_with_latest}'")
                                model_found = True
                        
                        if not model_found:
                            print(f"[WARNING] Attention : Le modele '{LLM_MODEL}' n'est pas dans la liste des modeles disponibles.")
                            if model_names:
                                print(f"[INFO] Suggestion : Utilisez l'un de ces modeles : {', '.join(model_names)}")
                except requests.exceptions.RequestException as e:
                    print(f"[WARNING] Impossible de se connecter a Ollama sur {OLLAMA_BASE_URL}")
                    print(f"   Erreur : {e}")
                    print(f"   Assurez-vous qu'Ollama est demarre : 'ollama serve'")
                
                self.llm = ChatOllama(
                    model=LLM_MODEL,
                    temperature=TEMPERATURE,
                    base_url=OLLAMA_BASE_URL
                )
            except Exception as e:
                print(f"[ERROR] Erreur lors de l'initialisation d'Ollama : {e}")
                raise
        else:
            # S'assurer que la cl√© API est dans l'environnement pour OpenAI
            if OPENAI_API_KEY:
                os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
            
            self.llm = ChatOpenAI(
                model=LLM_MODEL,
                temperature=TEMPERATURE
            )
        
        # Template de prompt syst√®me pour guider l'agent
        self.system_prompt = """Tu es un expert en agriculture intelligente et en gestion de l'irrigation.

Ta mission est d'analyser les donn√©es historiques d'irrigation, les conditions m√©t√©orologiques actuelles ET les donn√©es de capteurs IoT pour prendre une d√©cision √©clair√©e : IRRIGUER ou NE PAS IRRIGUER.

CRIT√àRES DE D√âCISION (par ordre de priorit√©) :
1. **HUMIDIT√â DU SOL** (CAPTEUR) - LE FACTEUR LE PLUS IMPORTANT :
   - < 25% = ALERTE CRITIQUE ‚Üí IRRIGUER IMM√âDIATEMENT
   - 25-30% = Sol sec ‚Üí IRRIGUER
   - 30-40% = Sol l√©g√®rement sec ‚Üí IRRIGUER si autres conditions favorables
   - 40-60% = Sol optimal ‚Üí NE PAS IRRIGUER sauf si √©vapotranspiration √©lev√©e
   - 60-70% = Sol bien hydrat√© ‚Üí NE PAS IRRIGUER
   - > 70% = Sol satur√© ‚Üí NE PAS IRRIGUER (risque de pourriture)

2. **NIVEAU DU R√âSERVOIR** (CAPTEUR) :
   - < 20% = Irrigation impossible ‚Üí NE PAS IRRIGUER (r√©servoir vide)
   - 20-30% = Niveau faible ‚Üí IRRIGUER seulement si sol tr√®s sec (< 25%)
   - > 30% = R√©servoir suffisant ‚Üí Peut irriguer si n√©cessaire

3. **√âVAPOTRANSPIRATION** (CAPTEUR) :
   - √âlev√©e (> 8 mm/jour) + sol sec ‚Üí IRRIGUER
   - Faible (< 3 mm/jour) ‚Üí Besoins r√©duits

4. **Conditions m√©t√©orologiques** :
   - Ne pas irriguer si pluviom√©trie r√©cente ou pr√©vue > 5mm
   - Ne pas irriguer si humidit√© de l'air > 80%
   - Temp√©rature √©lev√©e ‚Üí Besoins en eau augmentent

5. **Patterns historiques** :
   - Comparer avec cas similaires dans l'historique

R√àGLES IMPORTANTES :
- L'HUMIDIT√â DU SOL EST LE FACTEUR D√âCISIF - prioriser cette donn√©e
- Ne jamais irriguer si le r√©servoir est < 20%
- Ne pas irriguer si le sol est d√©j√† satur√© (> 70%)
- Prendre en compte les alertes des capteurs

FORMAT DE R√âPONSE :
Tu dois r√©pondre UNIQUEMENT avec un JSON valide au format suivant :
{{
    "decision": "IRRIGUER" ou "NE PAS IRRIGUER",
    "explication": "Une explication claire et concise en 2-3 phrases expliquant pourquoi cette d√©cision a √©t√© prise, en fran√ßais, adapt√©e pour un agriculteur. Mentionne sp√©cifiquement l'humidit√© du sol et le niveau du r√©servoir si disponibles."
}}
"""
    
    def make_decision(self, historical_summary: str, weather_summary: str, 
                     sensor_summary: str = "", sensor_alerts: list = None,
                     similar_cases: str = "") -> Dict:
        """
        Prend une d√©cision d'irrigation bas√©e sur les donn√©es fournies
        
        Args:
            historical_summary: R√©sum√© des donn√©es historiques
            weather_summary: R√©sum√© des conditions m√©t√©o actuelles
            sensor_summary: R√©sum√© des donn√©es de capteurs IoT
            sensor_alerts: Liste des alertes des capteurs
            similar_cases: Informations sur les cas similaires (optionnel)
        
        Returns:
            Dictionnaire contenant la d√©cision et l'explication
        """
        if sensor_alerts is None:
            sensor_alerts = []
        
        # Construction du message avec alertes
        alerts_text = ""
        if sensor_alerts:
            alerts_text = "\nüö® ALERTES DES CAPTEURS :\n" + "\n".join(sensor_alerts) + "\n"
        
        try:
            # Appel au LLM
            messages = [
                SystemMessage(content=self.system_prompt),
                HumanMessage(content=f"""
DONN√âES √Ä ANALYSER :
{historical_summary}

{weather_summary}

{sensor_summary}

{alerts_text}

{f"CAS SIMILAIRES DANS L'HISTORIQUE :\n{similar_cases}" if similar_cases else ""}

Prends maintenant ta d√©cision en analysant ces informations. PRIORISE les donn√©es de capteurs, surtout l'humidit√© du sol. R√©ponds UNIQUEMENT avec un JSON valide au format demand√©.""")
            ]
            
            response = self.llm.invoke(messages)
            
            # Extraction de la r√©ponse
            response_text = response.content.strip()
            
            # Nettoyage de la r√©ponse (enlever les markdown code blocks si pr√©sents)
            if response_text.startswith("```json"):
                response_text = response_text[7:]
            if response_text.startswith("```"):
                response_text = response_text[3:]
            if response_text.endswith("```"):
                response_text = response_text[:-3]
            response_text = response_text.strip()
            
            # Parsing du JSON
            decision_data = json.loads(response_text)
            
            # Validation
            if 'decision' not in decision_data or 'explication' not in decision_data:
                raise ValueError("Format de r√©ponse invalide")
            
            if decision_data['decision'] not in ['IRRIGUER', 'NE PAS IRRIGUER']:
                raise ValueError("D√©cision invalide")
            
            return decision_data
            
        except json.JSONDecodeError as e:
            print(f"Erreur de parsing JSON : {e}")
            print(f"R√©ponse re√ßue : {response_text}")
            # Fallback : essayer d'extraire la d√©cision manuellement
            return self._extract_decision_fallback(response_text)
        except Exception as e:
            error_msg = str(e)
            print(f"Erreur lors de la prise de d√©cision : {error_msg}")
            
            # Messages d'erreur plus explicites pour Ollama
            if 'not found' in error_msg.lower() or '404' in error_msg:
                error_msg = f"Mod√®le '{LLM_MODEL}' non trouv√© dans Ollama. V√©rifiez que le mod√®le est install√© avec 'ollama pull {LLM_MODEL}'"
            elif 'connection' in error_msg.lower() or 'refused' in error_msg.lower():
                error_msg = f"Impossible de se connecter √† Ollama sur {OLLAMA_BASE_URL}. Assurez-vous qu'Ollama est d√©marr√©."
            
            return {
                'decision': 'NE PAS IRRIGUER',
                'explication': f'Erreur lors de l\'analyse : {error_msg}. Par pr√©caution, l\'irrigation n\'est pas activ√©e.'
            }
    
    def _extract_decision_fallback(self, response_text: str) -> Dict:
        """
        M√©thode de fallback pour extraire la d√©cision si le JSON est mal format√©
        
        Args:
            response_text: Texte de r√©ponse du LLM
        
        Returns:
            Dictionnaire avec la d√©cision extraite
        """
        decision = 'NE PAS IRRIGUER'
        explication = response_text
        
        if 'IRRIGUER' in response_text.upper():
            decision = 'IRRIGUER'
        elif 'NE PAS IRRIGUER' in response_text.upper() or 'NON' in response_text.upper():
            decision = 'NE PAS IRRIGUER'
        
        return {
            'decision': decision,
            'explication': explication
        }

