"""
Script pour v√©rifier la configuration Ollama
"""
import requests
import sys
import io

# Configurer l'encodage UTF-8 pour Windows
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

OLLAMA_BASE_URL = "http://localhost:11434"

def check_ollama():
    """V√©rifie la disponibilit√© d'Ollama et liste les mod√®les"""
    print("üîç V√©rification de la configuration Ollama...")
    print("=" * 50)
    
    # V√©rifier si Ollama est accessible
    try:
        response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=5)
        if response.status_code == 200:
            print("‚úÖ Ollama est accessible")
        else:
            print(f"‚ùå Ollama r√©pond avec le code {response.status_code}")
            return False
    except requests.exceptions.ConnectionError:
        print(f"‚ùå Impossible de se connecter √† Ollama sur {OLLAMA_BASE_URL}")
        print("   Assurez-vous qu'Ollama est d√©marr√© : 'ollama serve'")
        return False
    except Exception as e:
        print(f"‚ùå Erreur : {e}")
        return False
    
    # Lister les mod√®les disponibles
    try:
        response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=5)
        data = response.json()
        models = data.get('models', [])
        
        if models:
            print(f"\nüìã Mod√®les disponibles ({len(models)}) :")
            for model in models:
                name = model.get('name', 'Inconnu')
                size = model.get('size', 0)
                size_gb = size / (1024**3) if size > 0 else 0
                print(f"   - {name} ({size_gb:.2f} GB)")
        else:
            print("\n‚ö†Ô∏è  Aucun mod√®le install√©")
            print("\nüí° Pour installer un mod√®le, utilisez :")
            print("   ollama pull llama3")
            print("   ou")
            print("   ollama pull llama2")
            print("   ou")
            print("   ollama pull mistral")
            return False
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la r√©cup√©ration des mod√®les : {e}")
        return False

def test_model(model_name):
    """Teste un mod√®le sp√©cifique"""
    print(f"\nüß™ Test du mod√®le '{model_name}'...")
    try:
        data = {
            'model': model_name,
            'prompt': 'R√©ponds simplement "OK"',
            'stream': False
        }
        response = requests.post(f"{OLLAMA_BASE_URL}/api/generate", json=data, timeout=30)
        
        if response.status_code == 200:
            result = response.json()
            print(f"‚úÖ Mod√®le '{model_name}' fonctionne correctement")
            return True
        else:
            error = response.json().get('error', 'Erreur inconnue')
            print(f"‚ùå Erreur : {error}")
            return False
    except Exception as e:
        print(f"‚ùå Erreur lors du test : {e}")
        return False

if __name__ == "__main__":
    if check_ollama():
        # Si un mod√®le est sp√©cifi√© en argument, le tester
        if len(sys.argv) > 1:
            model_name = sys.argv[1]
            test_model(model_name)
        else:
            print("\nüí° Pour tester un mod√®le sp√©cifique :")
            print("   python check_ollama.py <nom_modele>")
    else:
        print("\n‚ùå Configuration Ollama incompl√®te")
        sys.exit(1)

