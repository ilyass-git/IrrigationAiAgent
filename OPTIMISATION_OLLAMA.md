# üöÄ Guide d'Optimisation Ollama pour R√©duire le Temps de R√©ponse

## Probl√®me
Ollama peut prendre beaucoup de temps (plusieurs minutes) pour r√©pondre, ce qui ralentit le syst√®me d'irrigation.

## Solutions Impl√©ment√©es

### 1. ‚úÖ Timeout R√©duit
- **Avant** : 30 secondes
- **Maintenant** : 15 secondes (configurable via `OLLAMA_TIMEOUT`)
- Le syst√®me abandonne si Ollama ne r√©pond pas dans les 15 secondes

### 2. ‚úÖ Param√®tres de Performance Ollama
Les param√®tres suivants ont √©t√© ajout√©s pour acc√©l√©rer la g√©n√©ration :

- `num_predict=150` : Limite le nombre de tokens g√©n√©r√©s (r√©ponse plus courte = plus rapide)
- `num_ctx=2048` : R√©duit le contexte pour plus de vitesse
- `num_thread=4` : Utilise 4 threads pour le traitement
- `top_k=40` : R√©duit les options √† consid√©rer
- `top_p=0.9` : Sampling plus d√©terministe

### 3. ‚úÖ Prompt Optimis√©
- Prompt syst√®me raccourci de ~50%
- Instructions plus concises
- Moins de tokens √† traiter = r√©ponse plus rapide

## Configuration Recommand√©e

### Mod√®le Ollama
Pour de meilleures performances, utilisez un mod√®le plus petit et rapide :

```bash
# Mod√®les recommand√©s (du plus rapide au plus lent) :
ollama pull llama3.2:1b      # Tr√®s rapide, moins pr√©cis
ollama pull llama3.2:3b      # Rapide, bon compromis
ollama pull llama3:8b        # Moyen, bon √©quilibre
ollama pull llama3:latest    # Plus lent, plus pr√©cis (actuel)
```

**Recommandation** : Utilisez `llama3.2:3b` pour un bon √©quilibre vitesse/pr√©cision.

### Variables d'Environnement (.env)

```env
# Provider
LLM_PROVIDER=ollama

# Mod√®le (recommand√©: llama3.2:3b pour vitesse)
LLM_MODEL=llama3.2:3b

# Timeout (secondes) - r√©duit pour forcer des r√©ponses rapides
OLLAMA_TIMEOUT=15.0

# Param√®tres de performance
OLLAMA_NUM_PREDICT=150    # Limite les tokens g√©n√©r√©s (100-200 recommand√©)
OLLAMA_NUM_CTX=2048       # Contexte r√©duit (1024-4096 selon RAM)

# Temp√©rature (plus bas = plus d√©terministe = plus rapide)
TEMPERATURE=0.2
```

## Optimisations Suppl√©mentaires

### 1. Utiliser un Mod√®le Quantifi√©
Les mod√®les quantifi√©s sont plus rapides :

```bash
# Exemple avec Q4_K_M (quantification moyenne)
ollama pull llama3.2:3b-q4_K_M
```

### 2. Augmenter la RAM Allou√©e √† Ollama
Si vous avez assez de RAM, augmentez le contexte :

```env
OLLAMA_NUM_CTX=4096  # Au lieu de 2048
```

### 3. Utiliser GPU (si disponible)
Ollama utilise automatiquement le GPU s'il est disponible. V√©rifiez :

```bash
ollama show llama3.2:3b
# Cherchez "GPU" dans la sortie
```

### 4. R√©duire la Temp√©rature
Une temp√©rature plus basse = r√©ponses plus d√©terministes = plus rapides :

```env
TEMPERATURE=0.1  # Au lieu de 0.3
```

## Comparaison des Performances

| Mod√®le | Temps Moyen | Pr√©cision | RAM Requise |
|--------|-------------|-----------|-------------|
| llama3.2:1b | 2-5s | ‚≠ê‚≠ê | ~1GB |
| llama3.2:3b | 5-10s | ‚≠ê‚≠ê‚≠ê‚≠ê | ~2GB |
| llama3:8b | 10-20s | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ~5GB |
| llama3:latest | 20-60s+ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ~8GB |

## V√©rification

Pour tester les performances, regardez les logs :

```
[AGENT] ‚úì R√©ponse LLM re√ßue en X.XXs
```

Si c'est > 15s, le syst√®me utilisera le timeout et retournera une d√©cision s√©curis√©e.

## D√©pannage

### Ollama est toujours lent
1. V√©rifiez le mod√®le utilis√© : `ollama list`
2. Essayez un mod√®le plus petit : `ollama pull llama3.2:3b`
3. R√©duisez `OLLAMA_NUM_PREDICT` √† 100
4. V√©rifiez que le GPU est utilis√© (si disponible)

### Timeout trop court
Si vous obtenez souvent des timeouts :
1. Augmentez `OLLAMA_TIMEOUT` √† 20 ou 25 secondes
2. Utilisez un mod√®le plus rapide
3. V√©rifiez que votre CPU/GPU peut g√©rer le mod√®le

### R√©ponses incompl√®tes
Si le LLM ne retourne pas tous les champs :
- Le syst√®me compl√®te automatiquement avec des valeurs par d√©faut
- V√©rifiez les logs pour voir ce qui manque
- Augmentez l√©g√®rement `OLLAMA_NUM_PREDICT` si n√©cessaire


